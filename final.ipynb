{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "du3Wo4U4u42b"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "import time\n",
    "import catboost as cb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from bayes_opt import BayesianOptimization#pip install bayesian-optimization\n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "VWwECHrvu428",
    "outputId": "f862a636-5e15-4be9-b043-314216223118"
   },
   "outputs": [],
   "source": [
    "#df = reduce_mem_usage(pd.read_csv('train.csv'))\n",
    "#df_test = reduce_mem_usage(pd.read_csv('test.csv'))\n",
    "df = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df['acqic'].value_counts(dropna=False, normalize=True).head())\n",
    "for i,cn in enumerate(df[df.columns]):\n",
    "    print(df[cn].value_counts(dropna=False, normalize=True).sort_index())\n",
    "    print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJsMD7yHu43D"
   },
   "outputs": [],
   "source": [
    "#nan NY值處理 trainset testset\n",
    "#df['insfg'] = df['insfg'].map( {'Y': 1, 'N': 0} ).astype(int)#分期交易註記\n",
    "#df['ovrlt'] = df['ovrlt'].map( {'Y': 1, 'N': 0} ).astype(int)#超額註記碼\n",
    "#df['ecfg'] = df['ecfg'].map( {'Y': 1, 'N': 0} ).astype(int)#網路交易註記\n",
    "#df = df.drop(['flbmk','flg_3dsmk'],axis=1)\n",
    "#df_test = df_test.drop('flbmk',axis=1)\n",
    "#df_test= df_test.drop('flg_3dsmk',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#類別轉換\n",
    "#contp交易類別 csmcu消費地幣別 etymd交易型態 stocn消費地國別 scity hcefg支付形態  ?mchno特店代號\n",
    "#csmcu 消費地幣別     ovrlt超額註記碼 insfg  分期交易註記ecfg 網路交易註記\n",
    "\n",
    "#cat_cols = ['contp', 'csmcu', 'etymd', 'stocn', 'scity', 'hcefg', 'csmcu', 'ovrlt', 'insfg', 'ecfg']\n",
    "cat_cols =  list(df.columns)\n",
    "cat_cols.remove('fraud_ind')\n",
    "cat_cols.remove('txkey')\n",
    "cat_cols.remove('loctm')\n",
    "cat_cols.remove('locdt')\n",
    "cat_cols.remove('conam')\n",
    "for col in cat_cols:\n",
    "    if col in df.columns:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(df[col].astype(str).values) + list(df_test[col].astype(str).values))\n",
    "        df[col] = le.transform(list(df[col].astype(str).values))\n",
    "        df_test[col] = le.transform(list(df_test[col].astype(str).values)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if(df[col].duplicated().sum() <10):\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['fraud_ind'] = df['fraud_ind'].astype('category')\n",
    "#df_test['fraud_ind'] = df_test['fraud_ind'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "        df_test[col] = df_test[col].astype('category')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#若特徵中其中有一個值超過90%\n",
    "big_top_value_cols = [col for col in df.columns if df[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n",
    "big_top_value_cols_test = [col for col in df_test.columns if df_test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n",
    "print(big_top_value_cols )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.info()) #contp csmcu etymd stocn消費地國別 scity hcefg  csmcu   ovrlt insfg ecfg\n",
    "print(\"---------------------------\")\n",
    "print(df_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i,col in enumerate(df[df.columns]):\n",
    "#    df[col].fillna(-999, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_group_col ='mcc'\n",
    "groupby_col =['bacno','cano']\n",
    "df['mcc_to_mean_cred'] = df[by_group_col] / df.groupby(groupby_col)[by_group_col].transform('mean')\n",
    "df['mcc_to_std_cred'] = df[by_group_col] / df.groupby(groupby_col)[by_group_col].transform('std')\n",
    "df_test['mcc_to_mean_cred'] = df[by_group_col] / df.groupby(groupby_col)[by_group_col].transform('mean')\n",
    "df_test['mcc_to_std_cred'] = df[by_group_col] / df.groupby(groupby_col)[by_group_col].transform('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df.csv',index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_group_col ='conam'\n",
    "groupby_col =['bacno','cano']\n",
    "df['conam_to_mean_cred'] = df[by_group_col] / df.groupby(groupby_col)[by_group_col].transform('mean')\n",
    "df['conam_to_std_cred'] = df[by_group_col] / df.groupby(groupby_col)[by_group_col].transform('std')\n",
    "by_group_col ='conam'\n",
    "groupby_col =['acqic','stocn','scity','mcc','mchno']\n",
    "df['conam_to_mean_store'] = df[by_group_col] / df.groupby(groupby_col)[by_group_col].transform('mean')\n",
    "df['conam_to_std_store'] = df[by_group_col] / df.groupby(groupby_col)[by_group_col].transform('std')\n",
    "groupby_col =['insfg','iterm']\n",
    "df['conam_to_mean_staging'] = df[by_group_col] / df.groupby(groupby_col)[by_group_col].transform('mean')\n",
    "df['conam_to_std_staging'] = df[by_group_col] / df.groupby(groupby_col)[by_group_col].transform('std')\n",
    "groupby_col =['contp','hcefg','etymd']\n",
    "df['conam_to_mean_trade'] = df[by_group_col] / df.groupby(groupby_col)[by_group_col].transform('mean')\n",
    "df['conam_to_std_trade'] = df[by_group_col] / df.groupby(groupby_col)[by_group_col].transform('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 'fraud_ind'\n",
    "col_name ='conam_to_mean_store'\n",
    "cor = np.corrcoef(df[col_name], df[i])[0,1]\n",
    "df.loc[df['fraud_ind'] == 0].set_index(col_name)[i].fillna(-1).plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3), label=\"isFraud=0\")\n",
    "df.loc[df['fraud_ind'] == 1].set_index(col_name)[i].fillna(-1).plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3), label=\"isFraud=1\")\n",
    "#test_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "i = 'fraud_ind'\n",
    "col_name ='conam_to_mean_staging'\n",
    "cor = np.corrcoef(df[col_name], df[i])[0,1]\n",
    "df.loc[df['fraud_ind'] == 0].set_index(col_name)[i].fillna(-1).plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3), label=\"isFraud=0\")\n",
    "df.loc[df['fraud_ind'] == 1].set_index(col_name)[i].fillna(-1).plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3), label=\"isFraud=1\")\n",
    "#test_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "i = 'fraud_ind'\n",
    "col_name ='conam_to_mean_cred'\n",
    "cor = np.corrcoef(df[col_name], df[i])[0,1]\n",
    "df.loc[df['fraud_ind'] == 0].set_index(col_name)[i].fillna(-1).plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3), label=\"isFraud=0\")\n",
    "df.loc[df['fraud_ind'] == 1].set_index(col_name)[i].fillna(-1).plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3), label=\"isFraud=1\")\n",
    "#test_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "i = 'fraud_ind'\n",
    "col_name ='conam_to_mean_trade'\n",
    "cor = np.corrcoef(df[col_name], df[i])[0,1]\n",
    "df.loc[df['fraud_ind'] == 0].set_index(col_name)[i].fillna(-1).plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3), label=\"isFraud=0\")\n",
    "df.loc[df['fraud_ind'] == 1].set_index(col_name)[i].fillna(-1).plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3), label=\"isFraud=1\")\n",
    "#test_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['time'] = train['TransactionDT'] / (60*60*24) - 9/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainset的資料分布\n",
    "from matplotlib import gridspec\n",
    "plt.figure(figsize=(16,30*4))\n",
    "gs = gridspec.GridSpec(30, 1)#创建20行1列的画布\n",
    "for i, col  in enumerate(df[df.columns]):    \n",
    "    ax = plt.subplot(gs[i])\n",
    "    sns.distplot(df[col][df[\"fraud_ind\"] == 1],label = 'Is Fraud',bins=100)\n",
    "    sns.distplot(df[col][df[\"fraud_ind\"] == 0],label = 'Not Fraud',bins=100)\n",
    "    plt.legend();\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_title('histogram of feature: ' + str(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OUxAU5fmu43n",
    "outputId": "20021e6c-f205-4aa5-a9f4-b7c4c3ae8920"
   },
   "outputs": [],
   "source": [
    "print('No Frauds', round(df['fraud_ind'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(df['fraud_ind'].value_counts()[1]/len(df) * 100,2), '% of the dataset')\n",
    "df['fraud_ind'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#看分布圖處理\n",
    "droplist = ['txkey','locdt']\n",
    "df = df.drop(droplist,axis=1)\n",
    "df_test = df_test.drop(droplist,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_inf_nan(df):\n",
    "    return df.replace([np.inf, -np.inf], np.nan)   \n",
    "df = clean_inf_nan(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,cn in enumerate(df[df.columns]):\n",
    "    print(df[cn].value_counts(dropna=False, normalize=True).head())\n",
    "    print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#方案二 原始數據 無scaler\n",
    "X = df.drop(['fraud_ind'], axis = 1)\n",
    "Y = df['fraud_ind']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2, random_state = 10)\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "Y_train = Y_train.values\n",
    "Y_test = Y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4jMKegxou44X",
    "outputId": "9ae8f5f0-a7b4-46e7-e4db-b406e951f0c2"
   },
   "outputs": [],
   "source": [
    "# Let's store our Y_test legit and fraud counts for normalization purposes later on\n",
    "Y_test_transfer =pd.Series(Y_test)\n",
    "Y_test_nofraud = Y_test_transfer.value_counts()[0]\n",
    "Y_test_fraud = Y_test_transfer.value_counts()[1]\n",
    "print(Y_test_nofraud)\n",
    "from time import time\n",
    "t0=time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用隨機森林看特徵\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=50, max_depth=100, random_state=0,n_jobs=-1)\n",
    "rfc.fit(X_train,Y_train)  \n",
    "pred=rfc.predict(X_test)\n",
    "\n",
    "PlotConfusionMatrix(Y_test,pred,Y_test_nofraud,Y_test_fraud)\n",
    "\n",
    "x_feature = list(df.columns)\n",
    "x_feature.remove('fraud_ind')\n",
    "names = df[x_feature].columns\n",
    "\"\"\"for feature in zip(names, rfc.feature_importances_):\n",
    "    print(feature)\"\"\"\n",
    "\n",
    "#可视化由随机森林分类器判定的各类的重要顺序\n",
    "\n",
    "plt.style.use('fivethirtyeight')#其中的一种主题，可以通过plt.style.availabel查看有多少种主题\n",
    "#plt.rcParams['figure.figsize'] = (12,6)#设置画布尺寸\n",
    "importances = rfc.feature_importances_\n",
    "\n",
    "\n",
    "feat_names = names\n",
    "indices = np.argsort(importances)[::-1]#按照重要顺序从小到大排序并获取逆序索引\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.title(\"Feature importances by RandomTreeClassifier\")\n",
    "plt.bar(range(len(indices)), importances[indices], color='lightblue',  align=\"center\")\n",
    "plt.step(range(len(indices)), np.cumsum(importances[indices]), where='mid', label='Cumulative')\n",
    "plt.xticks(range(len(indices)), feat_names[indices], rotation='vertical',fontsize=14)\n",
    "plt.xlim([-1, len(indices)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feature = list(df.columns)\n",
    "x_feature.remove('fraud_ind')\n",
    "names = df[x_feature].columns\n",
    "for feature in zip(names, rfc.feature_importances_):\n",
    "    print(feature)\n",
    "\n",
    "#可视化由随机森林分类器判定的各类的重要顺序\n",
    "\n",
    "plt.style.use('fivethirtyeight')#其中的一种主题，可以通过plt.style.availabel查看有多少种主题\n",
    "#plt.rcParams['figure.figsize'] = (12,6)#设置画布尺寸\n",
    "importances = rfc.feature_importances_\n",
    "\n",
    "\n",
    "feat_names = names\n",
    "indices = np.argsort(importances)[::-1]#按照重要顺序从小到大排序并获取逆序索引\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.title(\"Feature importances by RandomTreeClassifier\")\n",
    "plt.bar(range(len(indices)), importances[indices], color='lightblue',  align=\"center\")\n",
    "plt.step(range(len(indices)), np.cumsum(importances[indices]), where='mid', label='Cumulative')\n",
    "plt.xticks(range(len(indices)), feat_names[indices], rotation='vertical',fontsize=14)\n",
    "plt.xlim([-1, len(indices)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#玩玩random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=1000, max_depth=100, random_state=0,n_jobs=-1)\n",
    "rfc.fit(X_train,Y_train)  \n",
    "pred=rfc.predict(X_test)\n",
    "\n",
    "PlotConfusionMatrix(Y_test,pred,Y_test_nofraud,Y_test_fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#随机森林删去多於特徵\n",
    "x_feature = list(df.columns)\n",
    "x_feature.remove('fraud_ind')\n",
    "x_feature.remove('insfg')\n",
    "x_feature.remove('iterm')\n",
    "x_feature.remove('contp')\n",
    "x_feature.remove('hcefg')\n",
    "x_feature.remove('ovrlt')\n",
    "x_feature.remove('mchno')\n",
    "x_feature.remove('scity')\n",
    "x_feature.remove('stscd')\n",
    "x_feature.remove('txkey')\n",
    "x_feature.remove('stocn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.drop(['fraud_ind'], axis = 1)\n",
    "Y_train = df['fraud_ind'].copy()\n",
    "X_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import sklearn.metrics as metrics\n",
    "def LGB_bayesian(\n",
    "    #learning_rate,\n",
    "    num_leaves, \n",
    "    bagging_fraction,\n",
    "    feature_fraction,\n",
    "    min_child_weight, \n",
    "    min_data_in_leaf,\n",
    "    max_depth,\n",
    "    reg_alpha,\n",
    "    reg_lambda\n",
    "     ):\n",
    "    \n",
    "    folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "    oof = np.zeros(len(X_train))\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, Y_train)):\n",
    "        print(\"fold n°{}\".format(fold_))\n",
    "        train_df, y_train_df = X_train.iloc[trn_idx], Y_train.iloc[trn_idx]\n",
    "        valid_df, y_valid_df = X_train.iloc[val_idx], Y_train.iloc[val_idx]\n",
    "    \n",
    "        trn_data = lgb.Dataset(train_df, label=y_train_df)\n",
    "        val_data = lgb.Dataset(valid_df, label=y_valid_df)\n",
    "         # LightGBM expects next three parameters need to be integer. \n",
    "        num_leaves = int(num_leaves)\n",
    "        min_data_in_leaf = int(min_data_in_leaf)\n",
    "        max_depth = int(max_depth)\n",
    "\n",
    "        assert type(num_leaves) == int\n",
    "        assert type(min_data_in_leaf) == int\n",
    "        assert type(max_depth) == int\n",
    "        param = {\n",
    "              'num_leaves': num_leaves, \n",
    "              'min_data_in_leaf': min_data_in_leaf,\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'bagging_fraction' : bagging_fraction,\n",
    "              'feature_fraction' : feature_fraction,\n",
    "              #'learning_rate' : learning_rate,\n",
    "              'max_depth': max_depth,\n",
    "              'reg_alpha': reg_alpha,\n",
    "              'reg_lambda': reg_lambda,\n",
    "              'objective': 'binary',\n",
    "              'save_binary': True,\n",
    "              'seed': 1337,\n",
    "              'feature_fraction_seed': 1337,\n",
    "              'bagging_seed': 1337,\n",
    "              'drop_seed': 1337,\n",
    "              'data_random_seed': 1337,\n",
    "              'boosting_type': 'gbdt',\n",
    "              'verbose': 1,\n",
    "              'is_unbalance': True,\n",
    "              'boost_from_average': True,\n",
    "              'metric':'auc'}    \n",
    "    \n",
    "        clf = lgb.train(param,\n",
    "                        trn_data,\n",
    "                        num_boost_round=50,\n",
    "                        valid_sets = [trn_data, val_data],\n",
    "                        verbose_eval=0,\n",
    "                        early_stopping_rounds = 200)\n",
    "        pred = clf.predict(valid_df,um_iteration=clf.best_iteration)\n",
    "        oof[val_idx] = pred\n",
    "       # oof[val_idx] = clf.predict(X_train.iloc[val_idx],\n",
    "        #                           num_iteration=clf.best_iteration)\n",
    "        \n",
    "      \n",
    "        #score = roc_auc_score(Y_train.iloc[trn_idx], oof[val_idx])\n",
    "    return metrics.roc_auc_score(y_valid_df, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounded region of parameter space\n",
    "bounds_LGB = {\n",
    "    'num_leaves': (31, 600), \n",
    "    'min_data_in_leaf': (20, 200),\n",
    "    'bagging_fraction' : (0.1, 0.9),\n",
    "    'feature_fraction' : (0.1, 0.9),\n",
    "    #'learning_rate': (0.01, 0.3),\n",
    "    'min_child_weight': (0.00001, 0.01),   \n",
    "    'reg_alpha': (1, 2), \n",
    "    'reg_lambda': (1, 2),\n",
    "    'max_depth':(-1,60),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LGB_BO.space.keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n",
    "init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "init_points = 10 #init_points表示初始点，n_iter代表迭代次数（即采样数）\n",
    "n_iter = 15\n",
    "\n",
    "print('-' * 130)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看最优化的score\n",
    "print(LGB_BO.max['target'])\n",
    " \n",
    "#查看优化得到的参数\n",
    "print(LGB_BO.max['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 499,\n",
    "          'min_child_weight': 0.009009297771374483,\n",
    "          'feature_fraction': 0.8931730501715401,\n",
    "          'bagging_fraction': 0.8249551970384116,\n",
    "          'min_data_in_leaf': 120,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': 47,\n",
    "          'learning_rate': 0.1,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'auc',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 1.1690935357787136,\n",
    "          'reg_lambda': 1.119698800271026,\n",
    "          'is_unbalance': True,\n",
    "          'random_state': 1337,  \n",
    "          'device': 'cpu',\n",
    "          #'gpu_platform_id': 1,\n",
    "          #'gpu_device_id': 0\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's auc: 0.999922\tvalid_1's auc: 0.988229\n",
      "[400]\ttraining's auc: 0.999996\tvalid_1's auc: 0.988523\n",
      "[600]\ttraining's auc: 0.999999\tvalid_1's auc: 0.988798\n",
      "[800]\ttraining's auc: 0.999999\tvalid_1's auc: 0.988824\n",
      "Early stopping, best iteration is:\n",
      "[662]\ttraining's auc: 0.999999\tvalid_1's auc: 0.988855\n",
      "  auc =  0.9888548883141521\n",
      "  auc =  0.90094060301936\n",
      "  f1 =  0.8009127210496291\n",
      "  confusion_matrix = \n",
      " [[374141   1072]\n",
      " [  1022   4212]]\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's auc: 0.99992\tvalid_1's auc: 0.989797\n",
      "[400]\ttraining's auc: 0.999995\tvalid_1's auc: 0.990346\n",
      "[600]\ttraining's auc: 0.999998\tvalid_1's auc: 0.990349\n",
      "Early stopping, best iteration is:\n",
      "[595]\ttraining's auc: 0.999998\tvalid_1's auc: 0.990371\n",
      "  auc =  0.9903712324529074\n",
      "  auc =  0.9016802325629625\n",
      "  f1 =  0.7970713366973384\n",
      "  confusion_matrix = \n",
      " [[374368   1083]\n",
      " [   968   4028]]\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's auc: 0.99992\tvalid_1's auc: 0.990696\n",
      "[400]\ttraining's auc: 0.999996\tvalid_1's auc: 0.990894\n",
      "Early stopping, best iteration is:\n",
      "[363]\ttraining's auc: 0.999994\tvalid_1's auc: 0.990956\n",
      "  auc =  0.9909556124440156\n",
      "  auc =  0.9029362484491944\n",
      "  f1 =  0.7797410942076917\n",
      "  confusion_matrix = \n",
      " [[373990   1360]\n",
      " [   971   4126]]\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's auc: 0.999924\tvalid_1's auc: 0.98764\n",
      "[400]\ttraining's auc: 0.999995\tvalid_1's auc: 0.98831\n",
      "[600]\ttraining's auc: 0.999999\tvalid_1's auc: 0.988228\n",
      "Early stopping, best iteration is:\n",
      "[432]\ttraining's auc: 0.999997\tvalid_1's auc: 0.988333\n",
      "  auc =  0.9883330705024623\n",
      "  auc =  0.8979375947840318\n",
      "  f1 =  0.7827781024741866\n",
      "  confusion_matrix = \n",
      " [[374198   1220]\n",
      " [  1010   4018]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = X_train.columns\n",
    "\n",
    "splits = 4\n",
    "folds = KFold(n_splits = splits)\n",
    "oof = np.zeros(len(X_train))\n",
    "predictions = np.zeros(len(X_test))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train.values, Y_train.values)):\n",
    "    print(\"Fold {}\".format(fold_))\n",
    "    train_df, y_train_df = X_train.iloc[trn_idx], Y_train.iloc[trn_idx]\n",
    "    valid_df, y_valid_df = X_train.iloc[val_idx], Y_train.iloc[val_idx]\n",
    "    \n",
    "    trn_data = lgb.Dataset(train_df, label=y_train_df)\n",
    "    val_data = lgb.Dataset(valid_df, label=y_valid_df)\n",
    "    \n",
    "    clf = lgb.train(params,\n",
    "                    trn_data,\n",
    "                    num_boost_round= 1000,\n",
    "                    valid_sets = [trn_data, val_data],\n",
    "                    verbose_eval=200,\n",
    "                    early_stopping_rounds = 200)\n",
    "    pred = clf.predict(valid_df)\n",
    "    oof[val_idx] = pred\n",
    "    \n",
    "    feature_importances['fold_{}'.format(fold_ + 1)] = clf.feature_importance()\n",
    "    \n",
    "    print( \"  auc = \", roc_auc_score(y_valid_df, pred) )\n",
    "    threshold = 0.5\n",
    "    y_pre = [int(item>threshold) for  item in pred]\n",
    "    print( \"  auc = \", roc_auc_score(y_valid_df, y_pre) )\n",
    "    print( \"  f1 = \", f1_score(y_valid_df, y_pre))\n",
    "    print( \"  confusion_matrix = \\r\\n\", confusion_matrix(y_valid_df, y_pre) )\n",
    "    predictions += clf.predict(X_test) / splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances['average'] = feature_importances.mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "sns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(30), x='average', y='feature')\n",
    "plt.title('TOP feature importance over cv folds average');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cHYcGd96u45M",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 结果保存\n",
    "threshold = 0.5\n",
    "predictions = [int(item>threshold) for  item in predictions]\n",
    "sampleSubmission = pd.read_csv('submission_test_sample.csv')\n",
    "sampleSubmission['fraud_ind'] = predictions\n",
    "sampleSubmission.to_csv('submission_test.csv',index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "y_pre = [int(item>threshold) for  item in pred]\n",
    "print( \"  auc = \", roc_auc_score(y_valid_df, y_pre))\n",
    "print( \"  f1 = \", f1_score(y_valid_df, y_pre))\n",
    "print( \"  confusion_matrix = \\r\\n\", confusion_matrix(y_valid_df, y_pre) )\n",
    "#predictions += clf.predict(X_test) / splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "colab_type": "code",
    "id": "0SNvyvj0u45I",
    "outputId": "681339fc-3f9e-402c-b4bc-42d6c95233bc"
   },
   "outputs": [],
   "source": [
    "x_feature = list(df.columns)\n",
    "x_feature.remove('fraud_ind')\n",
    "x_feature.remove('insfg')\n",
    "x_feature.remove('iterm')\n",
    "x_feature.remove('contp')\n",
    "x_feature.remove('hcefg')\n",
    "x_feature.remove('ovrlt')\n",
    "x_feature.remove('mchno')\n",
    "x_feature.remove('scity')\n",
    "x_feature.remove('stscd')\n",
    "x_feature.remove('txkey')\n",
    "x_feature.remove('stocn')\n",
    "df_test_final=df_test[x_feature]\n",
    "#預測(填答案)\n",
    "#若是decision tree只取重要特徵 要先做drop\n",
    "#df_test_d=df_test_initial[['acqic','bacno','cano','conam','csmcu','ecfg','locdt','mcc','scity','stocn','stscd']]\n",
    "result = gridcv.predict(df_test_final)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
